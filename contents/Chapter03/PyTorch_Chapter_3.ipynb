{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Transformations applied on each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # Input: 1 x 28 x 28, Output: 64 x 28 x 28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))  # Output: 64 x 14 x 14\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3),  # Output: 64 x 12 x 12\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))  # Output: 64 x 6 x 6\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)  # Flatten the output\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = FashionCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to train the model\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Train Epoch: {epoch} -- Loss: {loss.item():.6f}')\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(1, 50):  # Train for 15 epochs\n",
        "    train(epoch)\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')"
      ],
      "metadata": {
        "id": "fqnx2CntI_fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = FashionCNN().to(device)  # Assuming your model is already defined and moved to the device\n",
        "summary(model, input_size=(1, 28, 28))  # (Channels, Height, Width)\n"
      ],
      "metadata": {
        "id": "of337T5Jfeja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Horses or Humans"
      ],
      "metadata": {
        "id": "PuDPs_3tiy6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "url = \"https://storage.googleapis.com/learning-datasets/horse-or-human.zip\"\n",
        "file_name = \"horse-or-human.zip\"\n",
        "training_dir = 'horse-or-human/training/'\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()\n",
        "\n",
        "url = \"https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip\"\n",
        "file_name = \"validation-horse-or-human.zip\"\n",
        "validation_dir = 'horse-or-human/validation/'\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "zip_ref.extractall(validation_dir)\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "Nys-coaNiyWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((150,150)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomAffine(\n",
        "        degrees=0,  # No rotation\n",
        "        translate=(0.2, 0.2),  # Translate up to 20% vertically and horizontally\n",
        "        scale=(0.8, 1.2),  # Zoom in or out by 20%\n",
        "        shear=20,  # Shear by up to 20 degrees\n",
        "    ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset = datasets.ImageFolder(root=training_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=validation_dir, transform=train_transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Gq1cRZ4xir0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HorsesHumansCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HorsesHumansCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 18 * 18, 512)\n",
        "        self.drop = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(512, 1)  # Only 1 output neuron for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 18 * 18)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.sigmoid(x)  # Use sigmoid to output probabilities\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "vjb6kkGDjnD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HorsesHumansCNN().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device).float()  # Convert labels to float\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images).view(-1)  # Flatten outputs to match label shape\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "        # Evaluate on training set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device).float()\n",
        "                outputs = model(images).view(-1)\n",
        "                predicted = outputs > 0.5  # Threshold predictions\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print(f'Training Set Accuracy: {100 * correct / total}%')\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device).float()\n",
        "                outputs = model(images).view(-1)\n",
        "                predicted = outputs > 0.5  # Threshold predictions\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print(f'Validation Set Accuracy: {100 * correct / total}%')\n",
        "train_model(15)"
      ],
      "metadata": {
        "id": "-dEb0t61jrji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device).float()\n",
        "        outputs = model(images).view(-1)\n",
        "        predicted = outputs > 0.5  # Threshold predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        print(outputs)\n",
        "        print(labels)\n",
        "    print(f'Validation Accuracy: {100 * correct / total}%')"
      ],
      "metadata": {
        "id": "oLPYYyY2kwzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 150, 150))  # (Channels, Height, Width)"
      ],
      "metadata": {
        "id": "jaUbzC3W51Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((150, 150)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def load_image(image_path, transform):\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')  # Convert to RGB just in case it's not\n",
        "    # Apply transformations\n",
        "    image = transform(image)\n",
        "    # Add batch dimension, as the model expects batches\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "    # Prediction function\n",
        "def predict(image_path, model, device, transform):\n",
        "    model.eval()\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        prediction = output > 0.5\n",
        "        class_name = \"Human\" if prediction.item() == 1 else \"Horse\"\n",
        "        print(image_path)\n",
        "        print(f\"The image is predicted to be a {class_name}.\")\n",
        "        print(output)"
      ],
      "metadata": {
        "id": "XhssauRyh7dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for img in uploaded.keys():\n",
        "  predict(img, model, device, transform)\n"
      ],
      "metadata": {
        "id": "uZsB6CBthPir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n"
      ],
      "metadata": {
        "id": "8cNhqsQfvYO-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6Ukpodgw_PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "url = \"https://storage.googleapis.com/learning-datasets/horse-or-human.zip\"\n",
        "file_name = \"horse-or-human.zip\"\n",
        "training_dir = 'horse-or-human/training/'\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()\n",
        "\n",
        "url = \"https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip\"\n",
        "file_name = \"validation-horse-or-human.zip\"\n",
        "validation_dir = 'horse-or-human/validation/'\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "zip_ref.extractall(validation_dir)\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "gi0SrZQ9vw2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim import RMSprop\n",
        "\n",
        "# Load the pre-trained Inception V3 model\n",
        "pre_trained_model = models.inception_v3(pretrained=True, aux_logits=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model.to(device)\n",
        "\n",
        "def print_model_summary(model):\n",
        "    for name, module in model.named_modules():\n",
        "        print(f\"{name} : {module.__class__.__name__}\")\n",
        "\n",
        "\n",
        "# Example of how to use the function with your pre-trained model\n",
        "print_model_summary(pre_trained_model)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(pre_trained_model, input_size=(3, 299, 299))  # (Channels, Height, Width)"
      ],
      "metadata": {
        "id": "SigEpA17559g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers up to and including the 'Mixed_7c'\n",
        "for name, parameter in pre_trained_model.named_parameters():\n",
        "    parameter.requires_grad = False\n",
        "    if 'Mixed_7c' in name:\n",
        "        break\n",
        "\n",
        "# Modify the existing fully connected layer\n",
        "num_ftrs = pre_trained_model.fc.in_features\n",
        "pre_trained_model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1024),  # New fully connected layer with 1024 outputs\n",
        "    nn.ReLU(),                # Activation layer\n",
        "    nn.Linear(1024, 2)         # Final layer for binary classification\n",
        ")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Resize to match Inception V3 input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load datasets using ImageFolder\n",
        "train_dataset = ImageFolder(root=training_dir, transform=transform)\n",
        "val_dataset = ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs for training with auxiliary logits\n",
        "            if isinstance(outputs, tuple):\n",
        "                output, aux_output = outputs\n",
        "                loss1 = criterion(output, labels)\n",
        "                loss2 = criterion(aux_output, labels)\n",
        "                loss = loss1 + 0.4 * loss2  # Scale the auxiliary loss as is standard for Inception\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(output, 1)  # Ensure you use the main output for accuracy calculation\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qGM9T6yI1QjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only optimize parameters that are set to be trainable\n",
        "optimizer = RMSprop(filter(lambda p: p.requires_grad, pre_trained_model.parameters()), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(pre_trained_model, criterion, optimizer, train_loader, num_epochs=3)"
      ],
      "metadata": {
        "id": "w7ylKrDXGWTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total = 0\n",
        "    corrects = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs during evaluation\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # Use only the main output for evaluation\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += torch.sum(preds == labels).item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = corrects / total\n",
        "    print(f'Accuracy on the validation set: {accuracy:.4f} ({corrects}/{total})')\n",
        "    return accuracy\n",
        "\n",
        "# Assuming the necessary imports and pre_trained_model are defined and set up\n",
        "# Ensure the model and data loaders are on the appropriate device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model = pre_trained_model.to(device)\n",
        "\n",
        "# Assuming val_loader is defined and set up as previously shown\n",
        "\n",
        "accuracy = evaluate_model(pre_trained_model, val_loader, device)\n"
      ],
      "metadata": {
        "id": "wtzZYi2p4BfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cats versus Dogs\n",
        "(Note the following cells will only work if you have already run the above cells for training Horses v Humans)"
      ],
      "metadata": {
        "id": "xHnqIjfNEGEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O \"cats_and_dogs_filtered.zip\"\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"cats_and_dogs_filtered.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()\n",
        "\n",
        "training_dir = \"/tmp/cats_and_dogs_filtered/train/\"\n",
        "validation_dir = \"/tmp/cats_and_dogs_filtered/validation/\""
      ],
      "metadata": {
        "id": "dKUqD0cUEHxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim import RMSprop\n",
        "\n",
        "# Load the pre-trained Inception V3 model\n",
        "pre_trained_model = models.inception_v3(pretrained=True, aux_logits=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model.to(device)\n",
        "\n",
        "def print_model_summary(model):\n",
        "    for name, module in model.named_modules():\n",
        "        print(f\"{name} : {module.__class__.__name__}\")\n",
        "\n",
        "\n",
        "# Example of how to use the function with your pre-trained model\n",
        "print_model_summary(pre_trained_model)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(pre_trained_model, input_size=(3, 299, 299))  # (Channels, Height, Width)"
      ],
      "metadata": {
        "id": "96xx7VsnGjCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers up to and including the 'Mixed_7c'\n",
        "for name, parameter in pre_trained_model.named_parameters():\n",
        "    parameter.requires_grad = False\n",
        "    if 'Mixed_7c' in name:\n",
        "        break\n",
        "\n",
        "# Modify the existing fully connected layer\n",
        "num_ftrs = pre_trained_model.fc.in_features\n",
        "pre_trained_model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1024),  # New fully connected layer with 1024 outputs\n",
        "    nn.ReLU(),                # Activation layer\n",
        "    nn.Linear(1024, 2)         # Final layer for binary classification\n",
        ")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Resize to match Inception V3 input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load datasets using ImageFolder\n",
        "train_dataset = ImageFolder(root=training_dir, transform=transform)\n",
        "val_dataset = ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs for training with auxiliary logits\n",
        "            if isinstance(outputs, tuple):\n",
        "                output, aux_output = outputs\n",
        "                loss1 = criterion(output, labels)\n",
        "                loss2 = criterion(aux_output, labels)\n",
        "                loss = loss1 + 0.4 * loss2  # Scale the auxiliary loss as is standard for Inception\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(output, 1)  # Ensure you use the main output for accuracy calculation\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r1ED78kfFnFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only optimize parameters that are set to be trainable\n",
        "optimizer = RMSprop(filter(lambda p: p.requires_grad, pre_trained_model.parameters()), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(pre_trained_model, criterion, optimizer, train_loader, num_epochs=3)"
      ],
      "metadata": {
        "id": "qGUwiuwzHgom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total = 0\n",
        "    corrects = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs during evaluation\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # Use only the main output for evaluation\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += torch.sum(preds == labels).item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = corrects / total\n",
        "    print(f'Accuracy on the validation set: {accuracy:.4f} ({corrects}/{total})')\n",
        "    return accuracy\n",
        "\n",
        "# Assuming the necessary imports and pre_trained_model are defined and set up\n",
        "# Ensure the model and data loaders are on the appropriate device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model = pre_trained_model.to(device)\n",
        "\n",
        "# Assuming val_loader is defined and set up as previously shown\n",
        "\n",
        "accuracy = evaluate_model(pre_trained_model, val_loader, device)"
      ],
      "metadata": {
        "id": "DeH1rViALqPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def load_image(image_path, transform):\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')  # Convert to RGB just in case it's not\n",
        "    # Apply transformations\n",
        "    image = transform(image)\n",
        "    # Add batch dimension, as the model expects batches\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "    # Prediction function\n",
        "def predict(image_path, model, device, transform):\n",
        "    model.eval()\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        print(output)\n",
        "        prediction = torch.max(output, 1)\n",
        "        print(prediction)"
      ],
      "metadata": {
        "id": "zv4qoQBqL9Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for img in uploaded.keys():\n",
        "  predict(img, pre_trained_model, device, transform)"
      ],
      "metadata": {
        "id": "YEPsQDrUMNXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rock Paper Scissors"
      ],
      "metadata": {
        "id": "1rZh6cCkPOWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/rps.zip -O \"rps.zip\"\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"rps.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()\n",
        "\n",
        "training_dir = \"/tmp/rps/\""
      ],
      "metadata": {
        "id": "7iRGV8k1PNks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim import RMSprop\n",
        "\n",
        "# Load the pre-trained Inception V3 model\n",
        "pre_trained_model = models.inception_v3(pretrained=True, aux_logits=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model.to(device)\n",
        "\n",
        "def print_model_summary(model):\n",
        "    for name, module in model.named_modules():\n",
        "        print(f\"{name} : {module.__class__.__name__}\")\n",
        "\n",
        "\n",
        "# Example of how to use the function with your pre-trained model\n",
        "print_model_summary(pre_trained_model)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(pre_trained_model, input_size=(3, 299, 299))  # (Channels, Height, Width)"
      ],
      "metadata": {
        "id": "AZzlhyIoPuJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers up to and including the 'Mixed_7c'\n",
        "for name, parameter in pre_trained_model.named_parameters():\n",
        "    parameter.requires_grad = False\n",
        "    if 'Mixed_7c' in name:\n",
        "        break\n",
        "\n",
        "# Modify the existing fully connected layer\n",
        "num_ftrs = pre_trained_model.fc.in_features\n",
        "pre_trained_model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1024),  # New fully connected layer with 1024 outputs\n",
        "    nn.ReLU(),                # Activation layer\n",
        "    nn.Linear(1024, 3)         # Final layer for binary classification\n",
        ")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Resize to match Inception V3 input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load datasets using ImageFolder\n",
        "train_dataset = ImageFolder(root=training_dir, transform=transform)\n",
        "val_dataset = ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs for training with auxiliary logits\n",
        "            if isinstance(outputs, tuple):\n",
        "                output, aux_output = outputs\n",
        "                loss1 = criterion(output, labels)\n",
        "                loss2 = criterion(aux_output, labels)\n",
        "                loss = loss1 + 0.4 * loss2  # Scale the auxiliary loss as is standard for Inception\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(output, 1)  # Ensure you use the main output for accuracy calculation\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ofCmZ66EQCAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only optimize parameters that are set to be trainable\n",
        "optimizer = RMSprop(filter(lambda p: p.requires_grad, pre_trained_model.parameters()), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(pre_trained_model, criterion, optimizer, train_loader, num_epochs=3)"
      ],
      "metadata": {
        "id": "jMiFLJiBQKpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def load_image(image_path, transform):\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')  # Convert to RGB just in case it's not\n",
        "    # Apply transformations\n",
        "    image = transform(image)\n",
        "    # Add batch dimension, as the model expects batches\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "    # Prediction function\n",
        "def predict(image_path, model, device, transform):\n",
        "    model.eval()\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        print(output)\n",
        "        prediction = torch.max(output, 1)\n",
        "        print(prediction)"
      ],
      "metadata": {
        "id": "JkduW3KZaZYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for img in uploaded.keys():\n",
        "  predict(img, pre_trained_model, device, transform)"
      ],
      "metadata": {
        "id": "1GiZLEmMad4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}