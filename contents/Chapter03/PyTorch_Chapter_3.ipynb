{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqnx2CntI_fo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Transformations applied on each image\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # Convert images to tensor\n",
        "        transforms.Normalize((0.5,), (0.5,)),  # Normalize the images\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "\n",
        "# Define the CNN model\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                1, 64, kernel_size=3, padding=1\n",
        "            ),  # Input: 1 x 28 x 28, Output: 64 x 28 x 28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )  # Output: 64 x 14 x 14\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3),  # Output: 64 x 12 x 12\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )  # Output: 64 x 6 x 6\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)  # Flatten the output\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = FashionCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Function to train the model\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Train Epoch: {epoch} -- Loss: {loss.item():.6f}\")\n",
        "\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(1, 50):  # Train for 15 epochs\n",
        "    train(epoch)\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of337T5Jfeja"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = FashionCNN().to(\n",
        "    device\n",
        ")  # Assuming your model is already defined and moved to the device\n",
        "summary(model, input_size=(1, 28, 28))  # (Channels, Height, Width)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuDPs_3tiy6O"
      },
      "source": [
        "# Horses or Humans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nys-coaNiyWz"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "url = \"https://storage.googleapis.com/learning-datasets/horse-or-human.zip\"\n",
        "file_name = \"horse-or-human.zip\"\n",
        "training_dir = \"horse-or-human/training/\"\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, \"r\")\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip\"\n",
        "file_name = \"validation-horse-or-human.zip\"\n",
        "validation_dir = \"horse-or-human/validation/\"\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, \"r\")\n",
        "zip_ref.extractall(validation_dir)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gq1cRZ4xir0j"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_dir = \"horse-or-human/training/\"\n",
        "validation_dir = \"horse-or-human/validation/\"\n",
        "# Define transformations\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((150, 150)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomAffine(\n",
        "            degrees=0,  # No rotation\n",
        "            translate=(0.2, 0.2),  # Translate up to 20% vertically and horizontally\n",
        "            scale=(0.8, 1.2),  # Zoom in or out by 20%\n",
        "            shear=20,  # Shear by up to 20 degrees\n",
        "        ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset = datasets.ImageFolder(root=training_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=validation_dir, transform=train_transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vjb6kkGDjnD9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class HorsesHumansCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HorsesHumansCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 18 * 18, 512)\n",
        "        self.drop = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(512, 1)  # Only 1 output neuron for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 18 * 18)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.sigmoid(x)  # Use sigmoid to output probabilities\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-dEb0t61jrji"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.7057890449509476\n",
            "Training Set Accuracy: 69.4255111976631%\n",
            "Validation Set Accuracy: 50.0%\n",
            "Epoch 2, Loss: 0.5696496846097888\n",
            "Training Set Accuracy: 79.45472249269717%\n",
            "Validation Set Accuracy: 50.0%\n",
            "Epoch 3, Loss: 0.46444773041840753\n",
            "Training Set Accuracy: 72.15189873417721%\n",
            "Validation Set Accuracy: 50.0%\n",
            "Epoch 4, Loss: 0.4191161019332481\n",
            "Training Set Accuracy: 84.12852969814995%\n",
            "Validation Set Accuracy: 53.515625%\n",
            "Epoch 5, Loss: 0.34964124858379364\n",
            "Training Set Accuracy: 86.6601752677702%\n",
            "Validation Set Accuracy: 68.359375%\n",
            "Epoch 6, Loss: 0.26823490180752496\n",
            "Training Set Accuracy: 86.56280428432328%\n",
            "Validation Set Accuracy: 64.84375%\n",
            "Epoch 7, Loss: 0.20637338676235892\n",
            "Training Set Accuracy: 91.91820837390458%\n",
            "Validation Set Accuracy: 69.53125%\n",
            "Epoch 8, Loss: 0.21157201729489095\n",
            "Training Set Accuracy: 93.1840311587147%\n",
            "Validation Set Accuracy: 60.15625%\n",
            "Epoch 9, Loss: 0.17945410830504965\n",
            "Training Set Accuracy: 96.39727361246348%\n",
            "Validation Set Accuracy: 80.078125%\n",
            "Epoch 10, Loss: 0.22018039000756812\n",
            "Training Set Accuracy: 91.82083739045764%\n",
            "Validation Set Accuracy: 66.40625%\n",
            "Epoch 11, Loss: 0.17078033891139607\n",
            "Training Set Accuracy: 92.40506329113924%\n",
            "Validation Set Accuracy: 65.625%\n",
            "Epoch 12, Loss: 0.14394639816248056\n",
            "Training Set Accuracy: 95.71567672833496%\n",
            "Validation Set Accuracy: 61.328125%\n",
            "Epoch 13, Loss: 0.1211284025939125\n",
            "Training Set Accuracy: 87.14703018500487%\n",
            "Validation Set Accuracy: 51.5625%\n",
            "Epoch 14, Loss: 0.15302493276469636\n",
            "Training Set Accuracy: 92.11295034079845%\n",
            "Validation Set Accuracy: 78.90625%\n",
            "Epoch 15, Loss: 0.12774375943241245\n",
            "Training Set Accuracy: 96.68938656280429%\n",
            "Validation Set Accuracy: 68.359375%\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HorsesHumansCNN().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train_model(num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = (\n",
        "                images.to(device),\n",
        "                labels.to(device).float(),\n",
        "            )  # Convert labels to float\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images).view(-1)  # Flatten outputs to match label shape\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Evaluate on training set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device).float()\n",
        "                outputs = model(images).view(-1)\n",
        "                predicted = outputs > 0.5  # Threshold predictions\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print(f\"Training Set Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device).float()\n",
        "                outputs = model(images).view(-1)\n",
        "                predicted = outputs > 0.5  # Threshold predictions\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print(f\"Validation Set Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "\n",
        "train_model(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oLPYYyY2kwzX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([9.7736e-01, 9.9994e-01, 9.1021e-01, 9.8722e-01, 9.9984e-01, 7.7831e-01,\n",
            "        9.6096e-01, 8.3773e-02, 9.8293e-01, 9.9950e-01, 5.9853e-01, 9.8388e-01,\n",
            "        9.9975e-01, 9.9974e-01, 9.9774e-01, 9.7574e-01, 9.9871e-01, 4.4554e-01,\n",
            "        4.8558e-01, 9.9176e-01, 9.0314e-01, 9.0461e-01, 8.9361e-01, 6.1158e-01,\n",
            "        9.7448e-01, 9.9946e-01, 8.6108e-02, 9.9983e-01, 9.9262e-01, 9.9147e-01,\n",
            "        3.2320e-04, 2.8685e-02])\n",
            "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
            "tensor([0.9976, 0.6913, 0.9865, 0.2034, 0.4928, 0.9914, 0.9996, 0.9204, 0.2354,\n",
            "        0.8544, 0.9996, 0.9259, 0.9811, 0.7991, 0.9873, 0.7957, 0.9819, 0.6693,\n",
            "        0.9999, 0.9967, 0.9340, 0.9771, 0.0362, 0.7641, 0.9999, 0.9855, 0.9996,\n",
            "        0.9998, 0.9560, 0.9969, 0.7642, 0.1245])\n",
            "tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.])\n",
            "tensor([4.7083e-01, 9.9623e-01, 3.7660e-01, 9.9708e-01, 9.9443e-01, 8.8308e-01,\n",
            "        5.2275e-03, 9.9980e-01, 9.0329e-01, 9.8530e-01, 9.9743e-01, 9.8981e-01,\n",
            "        9.9880e-01, 9.7934e-01, 9.6534e-01, 9.9771e-01, 3.9217e-01, 9.9688e-01,\n",
            "        5.7677e-01, 9.9964e-01, 6.0841e-01, 8.6798e-01, 9.3644e-01, 2.6731e-01,\n",
            "        7.3983e-01, 9.9048e-01, 9.8882e-01, 8.8811e-06, 9.9905e-01, 9.9602e-01,\n",
            "        9.8316e-01, 6.5395e-04])\n",
            "tensor([0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.])\n",
            "tensor([0.9683, 0.9991, 0.9995, 0.9990, 0.9977, 0.4242, 0.9988, 0.9982, 0.9982,\n",
            "        0.4751, 0.9983, 0.8354, 0.9990, 0.7103, 0.9101, 0.2238, 0.9780, 0.9999,\n",
            "        0.9968, 0.9497, 0.9851, 0.9996, 0.9993, 0.3178, 0.4543, 0.9985, 0.9927,\n",
            "        0.8718, 0.9880, 0.9811, 0.2422, 0.9980])\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.])\n",
            "tensor([0.9969, 0.9986, 0.9013, 0.7660, 0.8012, 0.1528, 0.9960, 0.9968, 0.9983,\n",
            "        0.9993, 0.9745, 0.9496, 0.9536, 0.9980, 0.6711, 0.0017, 0.9990, 0.6344,\n",
            "        0.9978, 0.5413, 0.6180, 0.2489, 0.2892, 0.9598, 0.9893, 0.7475, 0.7328,\n",
            "        0.9994, 0.8062, 0.5599, 0.6255, 0.9518])\n",
            "tensor([1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
            "tensor([0.9894, 0.2233, 0.0090, 0.9993, 0.9882, 0.9572, 0.7111, 0.7423, 0.9984,\n",
            "        0.9992, 0.9388, 0.0147, 0.0686, 0.2368, 0.4605, 0.9456, 0.9953, 0.8862,\n",
            "        0.9997, 0.9987, 0.9998, 0.9582, 0.8527, 0.7954, 0.8266, 0.8835, 0.7655,\n",
            "        0.7147, 0.0856, 0.6929, 0.0520, 0.9932])\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
            "tensor([0.5798, 0.9617, 0.9798, 0.9928, 0.9999, 0.5894, 1.0000, 0.6537, 0.9995,\n",
            "        0.2720, 0.9981, 0.3895, 0.9211, 0.0585, 0.8382, 0.9790, 0.9813, 0.9938,\n",
            "        0.9781, 0.9963, 0.9306, 0.0187, 0.9910, 0.9994, 0.9999, 0.9992, 0.9994,\n",
            "        0.2675, 0.2838, 0.8686, 0.9086, 1.0000])\n",
            "tensor([0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.])\n",
            "tensor([0.5609, 0.9991, 0.1074, 0.2345, 0.9305, 0.9986, 0.9998, 0.9999, 0.9984,\n",
            "        0.9750, 0.9613, 0.9689, 0.3774, 0.9739, 0.0928, 0.9670, 0.9996, 0.7662,\n",
            "        1.0000, 0.9963, 0.8306, 0.5192, 0.0035, 0.9124, 0.9706, 0.0915, 0.2574,\n",
            "        0.9194, 0.9964, 0.9965, 0.6757, 0.8669])\n",
            "tensor([0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.])\n",
            "Validation Accuracy: 65.234375%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device).float()\n",
        "        outputs = model(images).view(-1)\n",
        "        predicted = outputs > 0.5  # Threshold predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        print(outputs)\n",
        "        print(labels)\n",
        "    print(f\"Validation Accuracy: {100 * correct / total}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jaUbzC3W51Oa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 150, 150]             448\n",
            "         MaxPool2d-2           [-1, 16, 75, 75]               0\n",
            "            Conv2d-3           [-1, 32, 75, 75]           4,640\n",
            "         MaxPool2d-4           [-1, 32, 37, 37]               0\n",
            "            Conv2d-5           [-1, 64, 37, 37]          18,496\n",
            "         MaxPool2d-6           [-1, 64, 18, 18]               0\n",
            "            Linear-7                  [-1, 512]      10,617,344\n",
            "           Dropout-8                  [-1, 512]               0\n",
            "            Linear-9                    [-1, 1]             513\n",
            "================================================================\n",
            "Total params: 10,641,441\n",
            "Trainable params: 10,641,441\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.26\n",
            "Forward/backward pass size (MB): 5.98\n",
            "Params size (MB): 40.59\n",
            "Estimated Total Size (MB): 46.83\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, input_size=(3, 150, 150))  # (Channels, Height, Width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XhssauRyh7dw"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((150, 150)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def load_image(image_path, transform):\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert(\n",
        "        \"RGB\"\n",
        "    )  # Convert to RGB just in case it's not\n",
        "    # Apply transformations\n",
        "    image = transform(image)\n",
        "    # Add batch dimension, as the model expects batches\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "    # Prediction function\n",
        "\n",
        "\n",
        "def predict(image_path, model, device, transform):\n",
        "    model.eval()\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        prediction = output > 0.5\n",
        "        class_name = \"Human\" if prediction.item() == 1 else \"Horse\"\n",
        "        print(image_path)\n",
        "        print(f\"The image is predicted to be a {class_name}.\")\n",
        "        print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZsB6CBthPir"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./test/girl-3277529_640.jpg\n",
            "The image is predicted to be a Human.\n",
            "tensor([[0.7271]])\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for img in uploaded.keys():\n",
        "#     predict(img, model, device, transform)\n",
        "imgs = [\"./test/horse-facts.jpg\", \"./test/M4ewc.jpg\", \"./test/girl-3277529_640.jpg\"]\n",
        "predict(imgs[2], model, device, train_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNhqsQfvYO-"
      },
      "source": [
        "# Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Ukpodgw_PN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi0SrZQ9vw2X"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "url = \"https://storage.googleapis.com/learning-datasets/horse-or-human.zip\"\n",
        "file_name = \"horse-or-human.zip\"\n",
        "training_dir = \"horse-or-human/training/\"\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, \"r\")\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()\n",
        "\n",
        "url = \"https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip\"\n",
        "file_name = \"validation-horse-or-human.zip\"\n",
        "validation_dir = \"horse-or-human/validation/\"\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, \"r\")\n",
        "zip_ref.extractall(validation_dir)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SigEpA17559g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim import RMSprop\n",
        "\n",
        "# Load the pre-trained Inception V3 model\n",
        "pre_trained_model = models.inception_v3(pretrained=True, aux_logits=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model.to(device)\n",
        "\n",
        "\n",
        "def print_model_summary(model):\n",
        "    for name, module in model.named_modules():\n",
        "        print(f\"{name} : {module.__class__.__name__}\")\n",
        "\n",
        "\n",
        "# Example of how to use the function with your pre-trained model\n",
        "print_model_summary(pre_trained_model)\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(pre_trained_model, input_size=(3, 299, 299))  # (Channels, Height, Width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qGM9T6yI1QjJ"
      },
      "outputs": [],
      "source": [
        "# Freeze all layers up to and including the 'Mixed_7c'\n",
        "for name, parameter in pre_trained_model.named_parameters():\n",
        "    parameter.requires_grad = False\n",
        "    if \"Mixed_7c\" in name:\n",
        "        break\n",
        "\n",
        "# Modify the existing fully connected layer\n",
        "num_ftrs = pre_trained_model.fc.in_features\n",
        "pre_trained_model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1024),  # New fully connected layer with 1024 outputs\n",
        "    nn.ReLU(),  # Activation layer\n",
        "    nn.Linear(1024, 2),  # Final layer for binary classification\n",
        ")\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((299, 299)),  # Resize to match Inception V3 input size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Load datasets using ImageFolder\n",
        "train_dataset = ImageFolder(root=training_dir, transform=transform)\n",
        "val_dataset = ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs for training with auxiliary logits\n",
        "            if isinstance(outputs, tuple):\n",
        "                output, aux_output = outputs\n",
        "                loss1 = criterion(output, labels)\n",
        "                loss2 = criterion(aux_output, labels)\n",
        "                loss = (\n",
        "                    loss1 + 0.4 * loss2\n",
        "                )  # Scale the auxiliary loss as is standard for Inception\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(\n",
        "                output, 1\n",
        "            )  # Ensure you use the main output for accuracy calculation\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "w7ylKrDXGWTZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Loss: 4.2148, Acc: 0.9455\n",
            "Epoch 2/3 - Loss: 3.5521, Acc: 0.9905\n",
            "Epoch 3/3 - Loss: 3.5704, Acc: 0.9820\n"
          ]
        }
      ],
      "source": [
        "# Only optimize parameters that are set to be trainable\n",
        "optimizer = RMSprop(\n",
        "    filter(lambda p: p.requires_grad, pre_trained_model.parameters()), lr=0.001\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(pre_trained_model, criterion, optimizer, train_loader, num_epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wtzZYi2p4BfX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the validation set: 0.9870 (987/1000)\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total = 0\n",
        "    corrects = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs during evaluation\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # Use only the main output for evaluation\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += torch.sum(preds == labels).item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = corrects / total\n",
        "    print(f\"Accuracy on the validation set: {accuracy:.4f} ({corrects}/{total})\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Assuming the necessary imports and pre_trained_model are defined and set up\n",
        "# Ensure the model and data loaders are on the appropriate device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model = pre_trained_model.to(device)\n",
        "\n",
        "# Assuming val_loader is defined and set up as previously shown\n",
        "\n",
        "accuracy = evaluate_model(pre_trained_model, val_loader, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHnqIjfNEGEC"
      },
      "source": [
        "# Cats versus Dogs\n",
        "(Note the following cells will only work if you have already run the above cells for training Horses v Humans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dKUqD0cUEHxY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-20 20:13:32--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 2404:6800:4004:801::201b, 2404:6800:4004:808::201b, 2404:6800:4004:827::201b, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|2404:6800:4004:801::201b|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘cats_and_dogs_filtered.zip’\n",
            "\n",
            "cats_and_dogs_filte 100%[===================>]  65.43M  25.6MB/s    in 2.6s    \n",
            "\n",
            "2025-07-20 20:13:35 (25.6 MB/s) - ‘cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O \"cats_and_dogs_filtered.zip\"\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"cats_and_dogs_filtered.zip\", \"r\")\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()\n",
        "\n",
        "training_dir = \"/tmp/cats_and_dogs_filtered/train/\"\n",
        "validation_dir = \"/tmp/cats_and_dogs_filtered/validation/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96xx7VsnGjCq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim import RMSprop\n",
        "\n",
        "# Load the pre-trained Inception V3 model\n",
        "pre_trained_model = models.inception_v3(pretrained=True, aux_logits=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model.to(device)\n",
        "\n",
        "\n",
        "def print_model_summary(model):\n",
        "    for name, module in model.named_modules():\n",
        "        print(f\"{name} : {module.__class__.__name__}\")\n",
        "\n",
        "\n",
        "# Example of how to use the function with your pre-trained model\n",
        "print_model_summary(pre_trained_model)\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(pre_trained_model, input_size=(3, 299, 299))  # (Channels, Height, Width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1ED78kfFnFn"
      },
      "outputs": [],
      "source": [
        "# Freeze all layers up to and including the 'Mixed_7c'\n",
        "for name, parameter in pre_trained_model.named_parameters():\n",
        "    parameter.requires_grad = False\n",
        "    if \"Mixed_7c\" in name:\n",
        "        break\n",
        "\n",
        "# Modify the existing fully connected layer\n",
        "num_ftrs = pre_trained_model.fc.in_features\n",
        "pre_trained_model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1024),  # New fully connected layer with 1024 outputs\n",
        "    nn.ReLU(),  # Activation layer\n",
        "    nn.Linear(1024, 2),  # Final layer for binary classification\n",
        ")\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((299, 299)),  # Resize to match Inception V3 input size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Load datasets using ImageFolder\n",
        "train_dataset = ImageFolder(root=training_dir, transform=transform)\n",
        "val_dataset = ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs for training with auxiliary logits\n",
        "            if isinstance(outputs, tuple):\n",
        "                output, aux_output = outputs\n",
        "                loss1 = criterion(output, labels)\n",
        "                loss2 = criterion(aux_output, labels)\n",
        "                loss = (\n",
        "                    loss1 + 0.4 * loss2\n",
        "                )  # Scale the auxiliary loss as is standard for Inception\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(\n",
        "                output, 1\n",
        "            )  # Ensure you use the main output for accuracy calculation\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGUwiuwzHgom"
      },
      "outputs": [],
      "source": [
        "# Only optimize parameters that are set to be trainable\n",
        "optimizer = RMSprop(\n",
        "    filter(lambda p: p.requires_grad, pre_trained_model.parameters()), lr=0.001\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(pre_trained_model, criterion, optimizer, train_loader, num_epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeH1rViALqPp"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total = 0\n",
        "    corrects = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs during evaluation\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # Use only the main output for evaluation\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += torch.sum(preds == labels).item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = corrects / total\n",
        "    print(f\"Accuracy on the validation set: {accuracy:.4f} ({corrects}/{total})\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Assuming the necessary imports and pre_trained_model are defined and set up\n",
        "# Ensure the model and data loaders are on the appropriate device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model = pre_trained_model.to(device)\n",
        "\n",
        "# Assuming val_loader is defined and set up as previously shown\n",
        "\n",
        "accuracy = evaluate_model(pre_trained_model, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv4qoQBqL9Xp"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def load_image(image_path, transform):\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert(\n",
        "        \"RGB\"\n",
        "    )  # Convert to RGB just in case it's not\n",
        "    # Apply transformations\n",
        "    image = transform(image)\n",
        "    # Add batch dimension, as the model expects batches\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "    # Prediction function\n",
        "\n",
        "\n",
        "def predict(image_path, model, device, transform):\n",
        "    model.eval()\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        print(output)\n",
        "        prediction = torch.max(output, 1)\n",
        "        print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEPsQDrUMNXh"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for img in uploaded.keys():\n",
        "    predict(img, pre_trained_model, device, transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rZh6cCkPOWg"
      },
      "source": [
        "# Rock Paper Scissors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iRGV8k1PNks"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/rps.zip -O \"rps.zip\"\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"rps.zip\", \"r\")\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()\n",
        "\n",
        "training_dir = \"/tmp/rps/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZzlhyIoPuJR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim import RMSprop\n",
        "\n",
        "# Load the pre-trained Inception V3 model\n",
        "pre_trained_model = models.inception_v3(pretrained=True, aux_logits=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pre_trained_model.to(device)\n",
        "\n",
        "\n",
        "def print_model_summary(model):\n",
        "    for name, module in model.named_modules():\n",
        "        print(f\"{name} : {module.__class__.__name__}\")\n",
        "\n",
        "\n",
        "# Example of how to use the function with your pre-trained model\n",
        "print_model_summary(pre_trained_model)\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(pre_trained_model, input_size=(3, 299, 299))  # (Channels, Height, Width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofCmZ66EQCAc"
      },
      "outputs": [],
      "source": [
        "# Freeze all layers up to and including the 'Mixed_7c'\n",
        "for name, parameter in pre_trained_model.named_parameters():\n",
        "    parameter.requires_grad = False\n",
        "    if \"Mixed_7c\" in name:\n",
        "        break\n",
        "\n",
        "# Modify the existing fully connected layer\n",
        "num_ftrs = pre_trained_model.fc.in_features\n",
        "pre_trained_model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1024),  # New fully connected layer with 1024 outputs\n",
        "    nn.ReLU(),  # Activation layer\n",
        "    nn.Linear(1024, 3),  # Final layer for binary classification\n",
        ")\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((299, 299)),  # Resize to match Inception V3 input size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Load datasets using ImageFolder\n",
        "train_dataset = ImageFolder(root=training_dir, transform=transform)\n",
        "val_dataset = ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            # Handle multiple outputs for training with auxiliary logits\n",
        "            if isinstance(outputs, tuple):\n",
        "                output, aux_output = outputs\n",
        "                loss1 = criterion(output, labels)\n",
        "                loss2 = criterion(aux_output, labels)\n",
        "                loss = (\n",
        "                    loss1 + 0.4 * loss2\n",
        "                )  # Scale the auxiliary loss as is standard for Inception\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(\n",
        "                output, 1\n",
        "            )  # Ensure you use the main output for accuracy calculation\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMiFLJiBQKpQ"
      },
      "outputs": [],
      "source": [
        "# Only optimize parameters that are set to be trainable\n",
        "optimizer = RMSprop(\n",
        "    filter(lambda p: p.requires_grad, pre_trained_model.parameters()), lr=0.001\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(pre_trained_model, criterion, optimizer, train_loader, num_epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkduW3KZaZYu"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def load_image(image_path, transform):\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert(\n",
        "        \"RGB\"\n",
        "    )  # Convert to RGB just in case it's not\n",
        "    # Apply transformations\n",
        "    image = transform(image)\n",
        "    # Add batch dimension, as the model expects batches\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "    # Prediction function\n",
        "\n",
        "\n",
        "def predict(image_path, model, device, transform):\n",
        "    model.eval()\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        print(output)\n",
        "        prediction = torch.max(output, 1)\n",
        "        print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GiZLEmMad4b"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for img in uploaded.keys():\n",
        "    predict(img, pre_trained_model, device, transform)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch-book",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
