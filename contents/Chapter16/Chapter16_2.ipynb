{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "fzhJZiGdA5C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "# Add gradient clipping\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "class PromptTuningBERT(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_virtual_tokens=50, max_length=512):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "        self.bert.requires_grad_(False)\n",
        "\n",
        "        self.n_tokens = num_virtual_tokens\n",
        "        self.max_length = max_length - num_virtual_tokens\n",
        "\n",
        "        vocab_size = self.bert.config.vocab_size\n",
        "        token_ids = torch.randint(0, vocab_size, (num_virtual_tokens,))\n",
        "        word_embeddings = self.bert.bert.embeddings.word_embeddings\n",
        "        prompt_embeddings = word_embeddings(token_ids).unsqueeze(0)\n",
        "        self.prompt_embeddings = nn.Parameter(prompt_embeddings)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        batch_size = input_ids.shape[0]\n",
        "        input_ids = input_ids[:, :self.max_length]\n",
        "        attention_mask = attention_mask[:, :self.max_length]\n",
        "\n",
        "        embeddings = self.bert.bert.embeddings.word_embeddings(input_ids)\n",
        "        prompt_embeddings = self.prompt_embeddings.expand(batch_size, -1, -1)\n",
        "        inputs_embeds = torch.cat([prompt_embeddings, embeddings], dim=1)\n",
        "\n",
        "        prompt_attention_mask = torch.ones(batch_size, self.n_tokens, device=attention_mask.device)\n",
        "        attention_mask = torch.cat([prompt_attention_mask, attention_mask], dim=1)\n",
        "\n",
        "        return self.bert(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "# Data preparation\n",
        "dataset = load_dataset(\"imdb\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "max_length = 512\n",
        "num_virtual_tokens = 20\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length - num_virtual_tokens\n",
        "    )\n",
        "\n",
        "# Use only 5000 examples for training\n",
        "train_size = 5000\n",
        "np.random.seed(42)\n",
        "train_indices = np.random.choice(len(dataset[\"train\"]), train_size, replace=False)\n",
        "test_indices = np.random.choice(len(dataset[\"test\"]), train_size, replace=False)\n",
        "\n",
        "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
        "tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n",
        "\n",
        "# Create subset for training\n",
        "tokenized_train = tokenized_train.select(train_indices)\n",
        "tokenized_test = tokenized_test.select(test_indices)\n",
        "\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_train, batch_size=64, shuffle=True)\n",
        "eval_dataloader = DataLoader(tokenized_test, batch_size=128)\n",
        "\n",
        "# Define the model\n",
        "model = PromptTuningBERT(num_virtual_tokens=num_virtual_tokens, max_length=max_length)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-2)\n",
        "num_epochs = 30\n",
        "\n",
        "# Perform the training\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f'Training Epoch {epoch + 1}'):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        labels = batch.pop('label')\n",
        "        outputs = model(**batch, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    val_accuracy = []\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc='Validating'):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            labels = batch.pop('label')\n",
        "\n",
        "            outputs = model(**batch, labels=labels)\n",
        "            total_val_loss += outputs.loss.item()\n",
        "\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            val_accuracy.extend((predictions == labels).cpu().numpy())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    avg_val_loss = total_val_loss / len(eval_dataloader)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}:\")\n",
        "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "torch.save(model.prompt_embeddings, \"imdb_prompt_embeddings.pt\")"
      ],
      "metadata": {
        "id": "ZfImgx9lObSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    'prompt_embeddings': model.prompt_embeddings,\n",
        "    'config': {\n",
        "        'num_virtual_tokens': model.n_tokens,\n",
        "        'model_name': 'bert-base-uncased'\n",
        "    }\n",
        "}, \"imdb_prompt_embeddings.pt\")"
      ],
      "metadata": {
        "id": "hvi43i-zJ0DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class PromptTunedBERTInference:\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", prompt_path=\"imdb_prompt_embeddings.pt\"):\n",
        "        # Set seeds for reproducibility\n",
        "        torch.manual_seed(42)\n",
        "        random.seed(42)\n",
        "        np.random.seed(42)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load saved state\n",
        "        try:\n",
        "            saved_state = torch.load(prompt_path, map_location=self.device)\n",
        "            if isinstance(saved_state, dict) and 'prompt_embeddings' in saved_state:\n",
        "                self.prompt_embeddings = saved_state['prompt_embeddings']\n",
        "                config = saved_state['config']\n",
        "                print(f\"Loaded config: {config}\")\n",
        "            else:\n",
        "                self.prompt_embeddings = saved_state  # Old format\n",
        "            print(f\"Loaded prompt embeddings shape: {self.prompt_embeddings.shape}\")\n",
        "            print(f\"Prompt embeddings sum: {self.prompt_embeddings.sum().item()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading prompt embeddings: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Initialize model and tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "        # Move to device and eval mode\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.prompt_embeddings = self.prompt_embeddings.to(self.device)\n",
        "\n",
        "        # Freeze everything\n",
        "        self.model.requires_grad_(False)\n",
        "\n",
        "    def predict(self, text):\n",
        "        with torch.no_grad():\n",
        "            # Tokenize\n",
        "            max_length = 512 - self.prompt_embeddings.shape[1]\n",
        "            inputs = self.tokenizer(text, padding=True, truncation=True,\n",
        "                                  max_length=max_length,\n",
        "                                  return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get embeddings\n",
        "            embeddings = self.model.bert.embeddings.word_embeddings(inputs['input_ids'])\n",
        "            batch_size = embeddings.shape[0]\n",
        "            prompt_embeds = self.prompt_embeddings.expand(batch_size, -1, -1)\n",
        "            inputs_embeds = torch.cat([prompt_embeds, embeddings], dim=1)\n",
        "\n",
        "            # Create attention mask\n",
        "            attention_mask = inputs['attention_mask']\n",
        "            prompt_attention = torch.ones(batch_size, self.prompt_embeddings.shape[1],\n",
        "                                        device=self.device)\n",
        "            attention_mask = torch.cat([prompt_attention, attention_mask], dim=1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(inputs_embeds=inputs_embeds,\n",
        "                               attention_mask=attention_mask)\n",
        "\n",
        "            # Get probabilities\n",
        "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            prediction = 1 - outputs.logits.argmax(-1).item()\n",
        "\n",
        "            return {\n",
        "                \"prediction\": prediction,\n",
        "                \"positive_prob\": probs[0][0].item(),\n",
        "                \"negative_prob\": probs[0][1].item(),\n",
        "                \"confidence\": probs[0][1 - prediction].item(),\n",
        "                \"logits\": outputs.logits[0].tolist()\n",
        "            }\n",
        "\n",
        "def test_consistency(model, num_runs=5):\n",
        "    test_review = \"This movie was absolutely brilliant! The acting was superb and the story kept me on the edge of my seat. A must-watch!\"\n",
        "    results = []\n",
        "\n",
        "    print(\"\\nTesting same review multiple times for consistency:\")\n",
        "    for i in range(num_runs):\n",
        "        result = model.predict(test_review)\n",
        "        sentiment = \"Positive\" if result['prediction'] == 1 else \"Negative\"\n",
        "        results.append({\n",
        "            'prediction': sentiment,\n",
        "            'positive_prob': result['positive_prob'],\n",
        "            'negative_prob': result['negative_prob'],\n",
        "            'logits': result['logits']\n",
        "        })\n",
        "        print(f\"\\nRun {i+1}\")\n",
        "        print(f\"Prediction: {sentiment}\")\n",
        "        print(f\"Logits: {result['logits']}\")\n",
        "        print(f\"Positive prob: {result['positive_prob']:.3f}\")\n",
        "        print(f\"Negative prob: {result['negative_prob']:.3f}\")\n",
        "\n",
        "    # Check consistency\n",
        "    predictions = [r['prediction'] for r in results]\n",
        "    logits = [r['logits'] for r in results]\n",
        "    print(\"\\nConsistency Check:\")\n",
        "    print(f\"Predictions same: {len(set(predictions)) == 1}\")\n",
        "    print(f\"Logits consistent: {all(logits[0] == l for l in logits)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = PromptTunedBERTInference()\n",
        "    test_consistency(model)"
      ],
      "metadata": {
        "id": "pgTEykAFZ6XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multiple_reviews(model):\n",
        "    test_reviews = [\n",
        "        {\n",
        "            \"text\": \"This movie was absolutely brilliant! The acting was superb and the story kept me on the edge of my seat. A must-watch!\",\n",
        "            \"expected\": \"Positive\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"What a complete waste of time. Bad acting, terrible plot, and the ending made no sense at all. Don't bother watching.\",\n",
        "            \"expected\": \"Negative\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"It was okay. Some good moments but nothing special. Probably wouldn't watch it again.\",\n",
        "            \"expected\": \"Mixed/Neutral\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"The best film I've seen this year! The cinematography was breathtaking and the script was perfect. Going to watch it again!\",\n",
        "            \"expected\": \"Positive\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Horrible experience. The theater was empty for a reason - this movie is just awful. Save your money.\",\n",
        "            \"expected\": \"Negative\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"A solid 7/10. Not groundbreaking but entertaining throughout. Good performances from the whole cast.\",\n",
        "            \"expected\": \"Positive\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"I went in with high expectations but left disappointed. The story had potential but failed to deliver.\",\n",
        "            \"expected\": \"Negative\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"A masterpiece of modern cinema. Every scene was perfectly crafted. The director outdid themselves.\",\n",
        "            \"expected\": \"Positive\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTesting multiple reviews:\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    for review in test_reviews:\n",
        "        result = model.predict(review['text'])\n",
        "        sentiment = \"Positive\" if result['prediction'] == 1 else \"Negative\"\n",
        "\n",
        "        print(f\"\\nReview: {review['text']}\")\n",
        "        print(f\"Expected sentiment: {review['expected']}\")\n",
        "        print(f\"Model prediction: {sentiment}\")\n",
        "        print(f\"Confidence scores:\")\n",
        "        print(f\"  Positive probability: {result['positive_prob']:.3f}\")\n",
        "        print(f\"  Negative probability: {result['negative_prob']:.3f}\")\n",
        "        print(f\"  Overall confidence: {result['confidence']:.3f}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = PromptTunedBERTInference()\n",
        "\n",
        "    # First test consistency\n",
        "    test_consistency(model, num_runs=3)\n",
        "\n",
        "    # Then test multiple reviews\n",
        "    test_multiple_reviews(model)"
      ],
      "metadata": {
        "id": "waz6xuwbKS1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}