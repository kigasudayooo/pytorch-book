{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "ft8qgdc0RpkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQsMiZqKRgC5"
      },
      "outputs": [],
      "source": [
        "# 1. Setup and Dependencies\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# 2. Load and Examine Data\n",
        "dataset = load_dataset(\"imdb\")  # Movie reviews for sentiment analysis\n",
        "print(f\"Train size: {len(dataset['train'])}\")\n",
        "print(f\"Test size: {len(dataset['test'])}\")\n",
        "\n",
        "# 3. Initialize Model and Tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 4. Preprocess Data\n",
        "def preprocess_function(examples):\n",
        "   result = tokenizer(\n",
        "       examples[\"text\"],\n",
        "       truncation=True,\n",
        "       max_length=512,\n",
        "       padding=True\n",
        "   )\n",
        "   result[\"labels\"] = examples[\"label\"]\n",
        "   return result\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "   preprocess_function,\n",
        "   batched=True,\n",
        "   remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset['train'][0])"
      ],
      "metadata": {
        "id": "o78e4EIcQpEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "cs7qHh8iQDnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Create Data Collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 6. Define Metrics\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# 7. Configure Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# 8. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 9. Train and Evaluate\n",
        "train_results = trainer.train()\n",
        "print(f\"\\nTraining results: {train_results}\")\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"\\nEvaluation results: {eval_results}\")\n",
        "\n",
        "# 10. Save Model\n",
        "trainer.save_model(\"./final_model\")\n",
        "\n",
        "# 11. Example Usage\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    positive_prob = predictions[0][1].item()\n",
        "    return {\n",
        "        'sentiment': 'positive' if positive_prob > 0.5 else 'negative',\n",
        "        'confidence': positive_prob if positive_prob > 0.5 else 1 - positive_prob\n",
        "    }\n",
        "\n",
        "# Test prediction\n",
        "test_text = \"This movie was absolutely fantastic! The acting was superb.\"\n",
        "result = predict_sentiment(test_text)\n",
        "print(f\"\\nTest prediction for '{test_text}':\")\n",
        "print(f\"Sentiment: {result['sentiment']}\")\n",
        "print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "\n",
        "# 12. Training Monitoring\n",
        "def plot_training_history():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Get training metrics from results\n",
        "    training_loss = train_results.metrics['train_loss']\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_loss)\n",
        "    plt.xlabel('Training Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.close()\n",
        "\n",
        "plot_training_history()"
      ],
      "metadata": {
        "id": "WTW1INHSPiKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}