{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNrWHYQfaNjC"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Basic sentiment analysis\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"I love working with transformers!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "text = generator(\"The future of AI is\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "I24JdYOXarUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation with specific parameters\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2\",\n",
        "    max_length=50,\n",
        "    temperature=0.7,\n",
        "    top_k=50\n",
        ")\n",
        "text = generator(\"The future of AI is\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "XPaonnc2aqdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load BERT tokenizer which uses WordPiece\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Let's use a sentence with some interesting words to tokenize\n",
        "text = \"The ultramarathoner prequalified for the immunohistochemistry conference in neuroscience.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Get the token IDs\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(\"\\nToken IDs:\", token_ids)\n",
        "\n",
        "# Decode back to show special tokens\n",
        "decoded = tokenizer.decode(token_ids)\n",
        "print(\"\\nDecoded with special tokens:\", decoded)\n",
        "\n",
        "# Let's see the mapping between tokens and their IDs\n",
        "print(\"\\nToken to ID mapping:\")\n",
        "for token in tokens:\n",
        "    # Convert single token to ID\n",
        "    id = tokenizer.convert_tokens_to_ids(token)\n",
        "    print(f\"{token:20} -> {id}\")"
      ],
      "metadata": {
        "id": "64i62j3v8G2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"The preprocessing workflow using tokenizers and tokenizers123 also has preprocessingABC and post-processing!\"\n",
        "\n",
        "tokens2 = tokenizer.tokenize(text2)\n",
        "print(\"Tokens:\", tokens2)"
      ],
      "metadata": {
        "id": "yQW6WhmrAZpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load GPT-2 tokenizer which uses BPE\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Same sentence as before\n",
        "text = \"The ultramarathoner prequalified for the immunohistochemistry conference in neuroscience.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Get the token IDs\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(\"\\nToken IDs:\", token_ids)\n",
        "\n",
        "# Decode back to show special tokens\n",
        "decoded = tokenizer.decode(token_ids)\n",
        "print(\"\\nDecoded with special tokens:\", decoded)\n",
        "\n",
        "# Show the mapping between tokens and their IDs\n",
        "print(\"\\nToken to ID mapping:\")\n",
        "for token in tokens:\n",
        "    id = tokenizer.convert_tokens_to_ids(token)\n",
        "    print(f\"{repr(token):20} -> {id}\")  # Using repr() to show the special characters\n",
        "\n",
        "# Let's also try a word with numbers and special characters\n",
        "text2 = \"In 2024, pre-processing costs $123.45!\"\n",
        "tokens2 = tokenizer.tokenize(text2)\n",
        "print(\"\\nExample 2 tokens:\", tokens2)"
      ],
      "metadata": {
        "id": "2iQBaqNoCFXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load T5 tokenizer which uses SentencePiece\n",
        "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
        "\n",
        "# Same sentence as before\n",
        "text = \"The ultramarathoner prequalified for the immunohistochemistry conference in neuroscience.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Get the token IDs\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(\"\\nToken IDs:\", token_ids)\n",
        "\n",
        "# Decode back to show special tokens\n",
        "decoded = tokenizer.decode(token_ids)\n",
        "print(\"\\nDecoded with special tokens:\", decoded)\n",
        "\n",
        "# Show the mapping between tokens and their IDs\n",
        "print(\"\\nToken to ID mapping:\")\n",
        "for token in tokens:\n",
        "    id = tokenizer.convert_tokens_to_ids(token)\n",
        "    print(f\"{repr(token):20} -> {id}\")\n",
        "\n",
        "# Let's also try a multilingual example with mixed scripts\n",
        "text2 = \"Tokyo 東京 is beautiful! Pre-processing in 2024 costs $123.45\"\n",
        "tokens2 = tokenizer.tokenize(text2)\n",
        "print(\"\\nMultilingual example tokens:\", tokens2)\n",
        "\n",
        "# Let's also look at how it handles whitespace and punctuation\n",
        "text3 = \"  Hello,  world!  \"  # Extra spaces\n",
        "tokens3 = tokenizer.tokenize(text3)\n",
        "print(\"\\nWhitespace handling example:\", tokens3)"
      ],
      "metadata": {
        "id": "o80LV-SuDhU8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}