{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZay5pyZMWBg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.292505  [    0/60000]\n",
            "loss: 0.569087  [ 6400/60000]\n",
            "loss: 0.563133  [12800/60000]\n",
            "loss: 0.498390  [19200/60000]\n",
            "loss: 0.437595  [25600/60000]\n",
            "loss: 0.626039  [32000/60000]\n",
            "loss: 0.570385  [38400/60000]\n",
            "loss: 0.354733  [44800/60000]\n",
            "loss: 0.511651  [51200/60000]\n",
            "loss: 0.532222  [57600/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.475519  [    0/60000]\n",
            "loss: 0.624383  [ 6400/60000]\n",
            "loss: 0.183673  [12800/60000]\n",
            "loss: 0.388223  [19200/60000]\n",
            "loss: 0.352381  [25600/60000]\n",
            "loss: 0.234361  [32000/60000]\n",
            "loss: 0.390089  [38400/60000]\n",
            "loss: 0.498336  [44800/60000]\n",
            "loss: 0.240499  [51200/60000]\n",
            "loss: 0.308524  [57600/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.460080  [    0/60000]\n",
            "loss: 0.266791  [ 6400/60000]\n",
            "loss: 0.205186  [12800/60000]\n",
            "loss: 0.291698  [19200/60000]\n",
            "loss: 0.328982  [25600/60000]\n",
            "loss: 0.287717  [32000/60000]\n",
            "loss: 0.323611  [38400/60000]\n",
            "loss: 0.412108  [44800/60000]\n",
            "loss: 0.525263  [51200/60000]\n",
            "loss: 0.349233  [57600/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.216862  [    0/60000]\n",
            "loss: 0.384143  [ 6400/60000]\n",
            "loss: 0.408085  [12800/60000]\n",
            "loss: 0.232149  [19200/60000]\n",
            "loss: 0.335631  [25600/60000]\n",
            "loss: 0.354285  [32000/60000]\n",
            "loss: 0.302923  [38400/60000]\n",
            "loss: 0.304136  [44800/60000]\n",
            "loss: 0.200742  [51200/60000]\n",
            "loss: 0.369357  [57600/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.369129  [    0/60000]\n",
            "loss: 0.257892  [ 6400/60000]\n",
            "loss: 0.224921  [12800/60000]\n",
            "loss: 0.398448  [19200/60000]\n",
            "loss: 0.425022  [25600/60000]\n",
            "loss: 0.271802  [32000/60000]\n",
            "loss: 0.369593  [38400/60000]\n",
            "loss: 0.421637  [44800/60000]\n",
            "loss: 0.276382  [51200/60000]\n",
            "loss: 0.240324  [57600/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.418450  [    0/60000]\n",
            "loss: 0.272711  [ 6400/60000]\n",
            "loss: 0.367202  [12800/60000]\n",
            "loss: 0.293111  [19200/60000]\n",
            "loss: 0.188128  [25600/60000]\n",
            "loss: 0.287307  [32000/60000]\n",
            "loss: 0.219062  [38400/60000]\n",
            "loss: 0.176627  [44800/60000]\n",
            "loss: 0.173684  [51200/60000]\n",
            "loss: 0.276127  [57600/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.229784  [    0/60000]\n",
            "loss: 0.262340  [ 6400/60000]\n",
            "loss: 0.213560  [12800/60000]\n",
            "loss: 0.318693  [19200/60000]\n",
            "loss: 0.196980  [25600/60000]\n",
            "loss: 0.182667  [32000/60000]\n",
            "loss: 0.262185  [38400/60000]\n",
            "loss: 0.361170  [44800/60000]\n",
            "loss: 0.391181  [51200/60000]\n",
            "loss: 0.223956  [57600/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.244916  [    0/60000]\n",
            "loss: 0.223874  [ 6400/60000]\n",
            "loss: 0.163503  [12800/60000]\n",
            "loss: 0.148129  [19200/60000]\n",
            "loss: 0.258463  [25600/60000]\n",
            "loss: 0.198655  [32000/60000]\n",
            "loss: 0.127042  [38400/60000]\n",
            "loss: 0.311616  [44800/60000]\n",
            "loss: 0.221180  [51200/60000]\n",
            "loss: 0.241040  [57600/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.111180  [    0/60000]\n",
            "loss: 0.182117  [ 6400/60000]\n",
            "loss: 0.275373  [12800/60000]\n",
            "loss: 0.327058  [19200/60000]\n",
            "loss: 0.195612  [25600/60000]\n",
            "loss: 0.298483  [32000/60000]\n",
            "loss: 0.288681  [38400/60000]\n",
            "loss: 0.140831  [44800/60000]\n",
            "loss: 0.236101  [51200/60000]\n",
            "loss: 0.261531  [57600/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.096903  [    0/60000]\n",
            "loss: 0.179272  [ 6400/60000]\n",
            "loss: 0.206891  [12800/60000]\n",
            "loss: 0.190569  [19200/60000]\n",
            "loss: 0.153964  [25600/60000]\n",
            "loss: 0.234705  [32000/60000]\n",
            "loss: 0.267141  [38400/60000]\n",
            "loss: 0.355576  [44800/60000]\n",
            "loss: 0.295827  [51200/60000]\n",
            "loss: 0.109200  [57600/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.166799  [    0/60000]\n",
            "loss: 0.212763  [ 6400/60000]\n",
            "loss: 0.183939  [12800/60000]\n",
            "loss: 0.206574  [19200/60000]\n",
            "loss: 0.361859  [25600/60000]\n",
            "loss: 0.279379  [32000/60000]\n",
            "loss: 0.263177  [38400/60000]\n",
            "loss: 0.274230  [44800/60000]\n",
            "loss: 0.223772  [51200/60000]\n",
            "loss: 0.254672  [57600/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.114080  [    0/60000]\n",
            "loss: 0.233395  [ 6400/60000]\n",
            "loss: 0.179151  [12800/60000]\n",
            "loss: 0.305645  [19200/60000]\n",
            "loss: 0.190010  [25600/60000]\n",
            "loss: 0.163007  [32000/60000]\n",
            "loss: 0.168855  [38400/60000]\n",
            "loss: 0.108611  [44800/60000]\n",
            "loss: 0.200481  [51200/60000]\n",
            "loss: 0.234126  [57600/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.126463  [    0/60000]\n",
            "loss: 0.329133  [ 6400/60000]\n",
            "loss: 0.244445  [12800/60000]\n",
            "loss: 0.126483  [19200/60000]\n",
            "loss: 0.171499  [25600/60000]\n",
            "loss: 0.207282  [32000/60000]\n",
            "loss: 0.256745  [38400/60000]\n",
            "loss: 0.189076  [44800/60000]\n",
            "loss: 0.143550  [51200/60000]\n",
            "loss: 0.112068  [57600/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.204812  [    0/60000]\n",
            "loss: 0.194285  [ 6400/60000]\n",
            "loss: 0.158182  [12800/60000]\n",
            "loss: 0.165371  [19200/60000]\n",
            "loss: 0.115827  [25600/60000]\n",
            "loss: 0.222942  [32000/60000]\n",
            "loss: 0.201037  [38400/60000]\n",
            "loss: 0.184936  [44800/60000]\n",
            "loss: 0.322970  [51200/60000]\n",
            "loss: 0.162422  [57600/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.170135  [    0/60000]\n",
            "loss: 0.152460  [ 6400/60000]\n",
            "loss: 0.122475  [12800/60000]\n",
            "loss: 0.093452  [19200/60000]\n",
            "loss: 0.269734  [25600/60000]\n",
            "loss: 0.205901  [32000/60000]\n",
            "loss: 0.274209  [38400/60000]\n",
            "loss: 0.148648  [44800/60000]\n",
            "loss: 0.224621  [51200/60000]\n",
            "loss: 0.230266  [57600/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.172978  [    0/60000]\n",
            "loss: 0.193071  [ 6400/60000]\n",
            "loss: 0.367753  [12800/60000]\n",
            "loss: 0.147747  [19200/60000]\n",
            "loss: 0.088815  [25600/60000]\n",
            "loss: 0.161444  [32000/60000]\n",
            "loss: 0.127520  [38400/60000]\n",
            "loss: 0.280694  [44800/60000]\n",
            "loss: 0.340538  [51200/60000]\n",
            "loss: 0.284235  [57600/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.110424  [    0/60000]\n",
            "loss: 0.036861  [ 6400/60000]\n",
            "loss: 0.256713  [12800/60000]\n",
            "loss: 0.177740  [19200/60000]\n",
            "loss: 0.154130  [25600/60000]\n",
            "loss: 0.100572  [32000/60000]\n",
            "loss: 0.173452  [38400/60000]\n",
            "loss: 0.162538  [44800/60000]\n",
            "loss: 0.181465  [51200/60000]\n",
            "loss: 0.154675  [57600/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.115898  [    0/60000]\n",
            "loss: 0.099496  [ 6400/60000]\n",
            "loss: 0.150400  [12800/60000]\n",
            "loss: 0.183968  [19200/60000]\n",
            "loss: 0.193286  [25600/60000]\n",
            "loss: 0.293530  [32000/60000]\n",
            "loss: 0.089346  [38400/60000]\n",
            "loss: 0.384617  [44800/60000]\n",
            "loss: 0.107771  [51200/60000]\n",
            "loss: 0.058201  [57600/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.291767  [    0/60000]\n",
            "loss: 0.200107  [ 6400/60000]\n",
            "loss: 0.145194  [12800/60000]\n",
            "loss: 0.181385  [19200/60000]\n",
            "loss: 0.099532  [25600/60000]\n",
            "loss: 0.129407  [32000/60000]\n",
            "loss: 0.083042  [38400/60000]\n",
            "loss: 0.381354  [44800/60000]\n",
            "loss: 0.251630  [51200/60000]\n",
            "loss: 0.388235  [57600/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.126157  [    0/60000]\n",
            "loss: 0.110575  [ 6400/60000]\n",
            "loss: 0.126650  [12800/60000]\n",
            "loss: 0.126673  [19200/60000]\n",
            "loss: 0.225175  [25600/60000]\n",
            "loss: 0.133879  [32000/60000]\n",
            "loss: 0.303102  [38400/60000]\n",
            "loss: 0.256572  [44800/60000]\n",
            "loss: 0.237662  [51200/60000]\n",
            "loss: 0.141311  [57600/60000]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the dataset\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor()  # Automatically converts to tensor and scales to [0, 1]\n",
        "    ]\n",
        ")\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10), nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = FashionMNISTModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Train the model\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "# Training process\n",
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_function, optimizer)\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozzKgn7_O_66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.334304 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to test the model\n",
        "def test(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_function(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(\n",
        "        f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test(test_loader, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNZ-FF6NPSTe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ -9.2031, -10.1131,  -7.9562,  -0.0204,  -4.2867,  -7.2513,  -5.2563,\n",
            "         -20.1080, -15.0160, -16.0607]])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model predicted 3, and the actual label is 3.\n"
          ]
        }
      ],
      "source": [
        "# CHECK SINGLE PREDICTION AND PLOT\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def predict_single_image(image, label, model):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Image needs to be unsqueezed as the model expects a batch dimension\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image)\n",
        "        print(prediction)\n",
        "        predicted_label = prediction.argmax(1).item()\n",
        "\n",
        "    # Display the image and predictions\n",
        "    plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "    plt.title(f\"Predicted: {predicted_label}, Actual: {label}\")\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "\n",
        "# Choose an image from the test set\n",
        "image, label = test_dataset[33]  # Change index to test different images\n",
        "\n",
        "# Predict the class for the chosen image\n",
        "predicted_label = predict_single_image(image, label, model)\n",
        "print(f\"The model predicted {predicted_label}, and the actual label is {label}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve9Y_VI-yaCq"
      },
      "outputs": [],
      "source": [
        "# EARLY STOPPING VERSION\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the dataset\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor()  # Automatically converts to tensor and scales to [0, 1]\n",
        "    ]\n",
        ")\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10), nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = FashionMNISTModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def get_accuracy(pred, labels):\n",
        "    _, predictions = torch.max(pred, 1)\n",
        "    correct = (predictions == labels).float().sum()\n",
        "    accuracy = correct / labels.shape[0]\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Train the model with accuracy reporting\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        accuracy = get_accuracy(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            avg_loss = total_loss / (batch + 1)\n",
        "            avg_accuracy = total_accuracy / (batch + 1) * 100\n",
        "            print(\n",
        "                f\"Batch {batch}, Loss: {avg_loss:>7f}, Accuracy: {avg_accuracy:>0.2f}% [{current:>5d}/{size:>5d}]\"\n",
        "            )\n",
        "\n",
        "    # Early stopping condition\n",
        "    if avg_accuracy >= 95:\n",
        "        print(\"Reached 95% accuracy, stopping training.\")\n",
        "        return True  # Stop training\n",
        "\n",
        "\n",
        "# Training process\n",
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    if train(\n",
        "        train_loader, model, loss_function, optimizer\n",
        "    ):  # Check for the early stopping signal\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "print(\"Done!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch-book",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
